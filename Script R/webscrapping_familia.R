library(chromote)
library(rvest)
library(dplyr)
library(stringr)
library(tibble)
library(jsonlite)

# üîç Fonction d'extraction avec gestion des champs manquants
extract_projects <- function(page_html) {
  page <- read_html(page_html)
  inputs <- page %>% html_elements("input[name='numeroNotice']")

  json_list <- inputs %>%
    html_attr("value") %>%
    lapply(fromJSON)

  df <- bind_rows(json_list)

  # Ajout des colonnes manquantes si n√©cessaires
  expected_cols <- c(
    "Titre", "Auteurs", "date", "TypeDocument", "MotsCles",
    "Thematiques", "Disciplines", "TypesDocs", "Sommaire",
    "Notice", "T2", "VL", "IS", "SP", "URL"
  )
  for (col in expected_cols) {
    if (!col %in% names(df)) df[[col]] <- NA
  }

  df <- df %>%
    transmute(
      titre = Titre,
      auteurs = Auteurs,
      annee = date,
      type_doc = TypeDocument,
      mots_cles = MotsCles,
      thematiques = Thematiques,
      disciplines = Disciplines,
      types_document = TypesDocs,
      sommaire = Sommaire,
      reference = Notice,
      revue = T2,
      volume = VL,
      numero = IS,
      pages = SP,
      url = URL
    )

  return(df)
}

# üîÅ Fonction principale de scraping (toutes les pages)
scrape_all_pages <- function(base_url) {
  b <- ChromoteSession$new()
  b$Page$navigate(base_url)
  b$Page$loadEventFired()
  Sys.sleep(4)

  all_data <- list()
  page_num <- 1

  repeat {
    message("üìÑ Chargement de la page ", page_num, "...")

    tryCatch({
      b$Runtime$evaluate(
        expression = "
        new Promise(resolve => {
          const waitForResults = () => {
            const items = document.querySelectorAll('input[name=\\'numeroNotice\\']');
            if (items.length > 0) {
              resolve('ok');
            } else {
              setTimeout(waitForResults, 500);
            }
          };
          waitForResults();
        });
        "
      )
    }, error = function(e) {
      message("‚ùå Timeout lors de l‚Äôattente des donn√©es.")
    })

    Sys.sleep(1)

    html <- b$DOM$getDocument()
    node_id <- html$root$nodeId
    html_content <- b$DOM$getOuterHTML(nodeId = node_id)$outerHTML

    projects <- extract_projects(html_content)
    all_data[[page_num]] <- projects
    message("‚úÖ Page ", page_num, " r√©cup√©r√©e.")

    next_url <- tryCatch({
      b$Runtime$evaluate(
        expression = "
        (function() {
          const nextBtn = document.querySelector('a[data-ci-pagination-page][rel=\"next\"]');
          return nextBtn ? nextBtn.href : null;
        })();
        "
      )$result$value
    }, error = function(e) {
      message("‚ö†Ô∏è Impossible de d√©tecter l‚ÄôURL suivante."); NULL
    })

    if (is.null(next_url)) {
      message("üìå Fin : plus de page suivante.")
      break
    }

    tryCatch({
      message("‚û°Ô∏è Navigation vers : ", next_url)
      b$Page$navigate(next_url)
      b$Page$loadEventFired()
      Sys.sleep(5)
    }, error = function(e) {
      message("‚ùå √âchec navigation vers : ", next_url)
      break
    })

    page_num <- page_num + 1
  }

  b$close()
  final_data <- bind_rows(all_data)
  return(final_data)
}

# ‚ñ∂Ô∏è Lancer le scraping sur toutes les pages
base_url <- "https://familia.ucs.inrs.ca/resultat-de-recherche/?discipline[]=438"
raw_data <- scrape_all_pages(base_url)

# üëÅÔ∏è Aper√ßu des premi√®res lignes
print(head(raw_data, 5), width = Inf)

# üíæ Export CSV complet
write.csv(raw_data, "database/df_projets_familia.csv", row.names = FALSE)

